{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting quando la macchina impara a memoria!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video spiegazione grafica underfitting VS overfitting\n",
    "[video](https://www.youtube.com/watch?v=FHJfubAnos8&list=PLa-sizbCyh93evwIevvnjWFEH94N5giIG&index=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creiamo un dataset totalmente casuale\n",
    "In questo dataset è impossibile creare una funzione di interpolazione perchè i dati sono **totalmente** casuali. NON esiste un modello in grado di ***fittare*** i dati forniti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "X = np.random.random(size=(n, 5)) # matrice features 5 colonne 100 righe\n",
    "y = np.random.choice([\"si\", \"no\"], size = n) # target 100 risposte tra si e no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = MLPClassifier(hidden_layer_sizes=[1000, 500], verbose=2)\n",
    "# 1000 neuroni al primo strato, 500 al secondo, 2 in uscita (la predizione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70092022\n",
      "Iteration 2, loss = 0.69254288\n",
      "Iteration 3, loss = 0.68420878\n",
      "Iteration 4, loss = 0.67495348\n",
      "Iteration 5, loss = 0.67002145\n",
      "Iteration 6, loss = 0.66552924\n",
      "Iteration 7, loss = 0.65924448\n",
      "Iteration 8, loss = 0.65391956\n",
      "Iteration 9, loss = 0.65040644\n",
      "Iteration 10, loss = 0.64631767\n",
      "Iteration 11, loss = 0.64158856\n",
      "Iteration 12, loss = 0.63770577\n",
      "Iteration 13, loss = 0.63426136\n",
      "Iteration 14, loss = 0.63017877\n",
      "Iteration 15, loss = 0.62618550\n",
      "Iteration 16, loss = 0.62278743\n",
      "Iteration 17, loss = 0.61892130\n",
      "Iteration 18, loss = 0.61448428\n",
      "Iteration 19, loss = 0.61031965\n",
      "Iteration 20, loss = 0.60626142\n",
      "Iteration 21, loss = 0.60190095\n",
      "Iteration 22, loss = 0.59750930\n",
      "Iteration 23, loss = 0.59305071\n",
      "Iteration 24, loss = 0.58819987\n",
      "Iteration 25, loss = 0.58326851\n",
      "Iteration 26, loss = 0.57824429\n",
      "Iteration 27, loss = 0.57292184\n",
      "Iteration 28, loss = 0.56761309\n",
      "Iteration 29, loss = 0.56216922\n",
      "Iteration 30, loss = 0.55641489\n",
      "Iteration 31, loss = 0.55059909\n",
      "Iteration 32, loss = 0.54450304\n",
      "Iteration 33, loss = 0.53825984\n",
      "Iteration 34, loss = 0.53189959\n",
      "Iteration 35, loss = 0.52536431\n",
      "Iteration 36, loss = 0.51874372\n",
      "Iteration 37, loss = 0.51193950\n",
      "Iteration 38, loss = 0.50503083\n",
      "Iteration 39, loss = 0.49801897\n",
      "Iteration 40, loss = 0.49090206\n",
      "Iteration 41, loss = 0.48366928\n",
      "Iteration 42, loss = 0.47633689\n",
      "Iteration 43, loss = 0.46899789\n",
      "Iteration 44, loss = 0.46150445\n",
      "Iteration 45, loss = 0.45390022\n",
      "Iteration 46, loss = 0.44632953\n",
      "Iteration 47, loss = 0.43875840\n",
      "Iteration 48, loss = 0.43110295\n",
      "Iteration 49, loss = 0.42346918\n",
      "Iteration 50, loss = 0.41573738\n",
      "Iteration 51, loss = 0.40799425\n",
      "Iteration 52, loss = 0.40030000\n",
      "Iteration 53, loss = 0.39251498\n",
      "Iteration 54, loss = 0.38464717\n",
      "Iteration 55, loss = 0.37683293\n",
      "Iteration 56, loss = 0.36911458\n",
      "Iteration 57, loss = 0.36107278\n",
      "Iteration 58, loss = 0.35320106\n",
      "Iteration 59, loss = 0.34546881\n",
      "Iteration 60, loss = 0.33768345\n",
      "Iteration 61, loss = 0.33005382\n",
      "Iteration 62, loss = 0.32282987\n",
      "Iteration 63, loss = 0.31657224\n",
      "Iteration 64, loss = 0.31214362\n",
      "Iteration 65, loss = 0.30460903\n",
      "Iteration 66, loss = 0.29401746\n",
      "Iteration 67, loss = 0.28999018\n",
      "Iteration 68, loss = 0.28362779\n",
      "Iteration 69, loss = 0.27459009\n",
      "Iteration 70, loss = 0.27094374\n",
      "Iteration 71, loss = 0.26372960\n",
      "Iteration 72, loss = 0.25643472\n",
      "Iteration 73, loss = 0.25261132\n",
      "Iteration 74, loss = 0.24495482\n",
      "Iteration 75, loss = 0.23886597\n",
      "Iteration 76, loss = 0.23430149\n",
      "Iteration 77, loss = 0.22721306\n",
      "Iteration 78, loss = 0.22195859\n",
      "Iteration 79, loss = 0.21709926\n",
      "Iteration 80, loss = 0.21055468\n",
      "Iteration 81, loss = 0.20546920\n",
      "Iteration 82, loss = 0.20081801\n",
      "Iteration 83, loss = 0.19511009\n",
      "Iteration 84, loss = 0.18981826\n",
      "Iteration 85, loss = 0.18530340\n",
      "Iteration 86, loss = 0.18008706\n",
      "Iteration 87, loss = 0.17505955\n",
      "Iteration 88, loss = 0.17059273\n",
      "Iteration 89, loss = 0.16595719\n",
      "Iteration 90, loss = 0.16114615\n",
      "Iteration 91, loss = 0.15653667\n",
      "Iteration 92, loss = 0.15242032\n",
      "Iteration 93, loss = 0.14810850\n",
      "Iteration 94, loss = 0.14367573\n",
      "Iteration 95, loss = 0.13948109\n",
      "Iteration 96, loss = 0.13537084\n",
      "Iteration 97, loss = 0.13149998\n",
      "Iteration 98, loss = 0.12767075\n",
      "Iteration 99, loss = 0.12371415\n",
      "Iteration 100, loss = 0.11990192\n",
      "Iteration 101, loss = 0.11629202\n",
      "Iteration 102, loss = 0.11277679\n",
      "Iteration 103, loss = 0.10935482\n",
      "Iteration 104, loss = 0.10603790\n",
      "Iteration 105, loss = 0.10274804\n",
      "Iteration 106, loss = 0.09948657\n",
      "Iteration 107, loss = 0.09635107\n",
      "Iteration 108, loss = 0.09318026\n",
      "Iteration 109, loss = 0.09028288\n",
      "Iteration 110, loss = 0.08734968\n",
      "Iteration 111, loss = 0.08462495\n",
      "Iteration 112, loss = 0.08204907\n",
      "Iteration 113, loss = 0.07953394\n",
      "Iteration 114, loss = 0.07699702\n",
      "Iteration 115, loss = 0.07450550\n",
      "Iteration 116, loss = 0.07223233\n",
      "Iteration 117, loss = 0.06993043\n",
      "Iteration 118, loss = 0.06774555\n",
      "Iteration 119, loss = 0.06563834\n",
      "Iteration 120, loss = 0.06360071\n",
      "Iteration 121, loss = 0.06158977\n",
      "Iteration 122, loss = 0.05967625\n",
      "Iteration 123, loss = 0.05784154\n",
      "Iteration 124, loss = 0.05609918\n",
      "Iteration 125, loss = 0.05441551\n",
      "Iteration 126, loss = 0.05275722\n",
      "Iteration 127, loss = 0.05112737\n",
      "Iteration 128, loss = 0.04954276\n",
      "Iteration 129, loss = 0.04805342\n",
      "Iteration 130, loss = 0.04669580\n",
      "Iteration 131, loss = 0.04532590\n",
      "Iteration 132, loss = 0.04401890\n",
      "Iteration 133, loss = 0.04275940\n",
      "Iteration 134, loss = 0.04154422\n",
      "Iteration 135, loss = 0.04036449\n",
      "Iteration 136, loss = 0.03922800\n",
      "Iteration 137, loss = 0.03812311\n",
      "Iteration 138, loss = 0.03706659\n",
      "Iteration 139, loss = 0.03609596\n",
      "Iteration 140, loss = 0.03513269\n",
      "Iteration 141, loss = 0.03418296\n",
      "Iteration 142, loss = 0.03327240\n",
      "Iteration 143, loss = 0.03239804\n",
      "Iteration 144, loss = 0.03157985\n",
      "Iteration 145, loss = 0.03077106\n",
      "Iteration 146, loss = 0.02998563\n",
      "Iteration 147, loss = 0.02925610\n",
      "Iteration 148, loss = 0.02851789\n",
      "Iteration 149, loss = 0.02778948\n",
      "Iteration 150, loss = 0.02713951\n",
      "Iteration 151, loss = 0.02648286\n",
      "Iteration 152, loss = 0.02584906\n",
      "Iteration 153, loss = 0.02522242\n",
      "Iteration 154, loss = 0.02463583\n",
      "Iteration 155, loss = 0.02406241\n",
      "Iteration 156, loss = 0.02349244\n",
      "Iteration 157, loss = 0.02297793\n",
      "Iteration 158, loss = 0.02249552\n",
      "Iteration 159, loss = 0.02202439\n",
      "Iteration 160, loss = 0.02155672\n",
      "Iteration 161, loss = 0.02109849\n",
      "Iteration 162, loss = 0.02064381\n",
      "Iteration 163, loss = 0.02020778\n",
      "Iteration 164, loss = 0.01979360\n",
      "Iteration 165, loss = 0.01939446\n",
      "Iteration 166, loss = 0.01900340\n",
      "Iteration 167, loss = 0.01862576\n",
      "Iteration 168, loss = 0.01826938\n",
      "Iteration 169, loss = 0.01792011\n",
      "Iteration 170, loss = 0.01757246\n",
      "Iteration 171, loss = 0.01724980\n",
      "Iteration 172, loss = 0.01693572\n",
      "Iteration 173, loss = 0.01661741\n",
      "Iteration 174, loss = 0.01631217\n",
      "Iteration 175, loss = 0.01603009\n",
      "Iteration 176, loss = 0.01575290\n",
      "Iteration 177, loss = 0.01546556\n",
      "Iteration 178, loss = 0.01519561\n",
      "Iteration 179, loss = 0.01493668\n",
      "Iteration 180, loss = 0.01469156\n",
      "Iteration 181, loss = 0.01444465\n",
      "Iteration 182, loss = 0.01420992\n",
      "Iteration 183, loss = 0.01398910\n",
      "Iteration 184, loss = 0.01376086\n",
      "Iteration 185, loss = 0.01354422\n",
      "Iteration 186, loss = 0.01333782\n",
      "Iteration 187, loss = 0.01313348\n",
      "Iteration 188, loss = 0.01293660\n",
      "Iteration 189, loss = 0.01274623\n",
      "Iteration 190, loss = 0.01255514\n",
      "Iteration 191, loss = 0.01236497\n",
      "Iteration 192, loss = 0.01218725\n",
      "Iteration 193, loss = 0.01201877\n",
      "Iteration 194, loss = 0.01184841\n",
      "Iteration 195, loss = 0.01167447\n",
      "Iteration 196, loss = 0.01150964\n",
      "Iteration 197, loss = 0.01134959\n",
      "Iteration 198, loss = 0.01119243\n",
      "Iteration 199, loss = 0.01104297\n",
      "Iteration 200, loss = 0.01089918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "p_train = model.predict(X_train)\n",
    "p_test = model.predict(X_test)\n",
    "acc_train = accuracy_score(y_train, p_train)\n",
    "acc_test = accuracy_score(y_test, p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1.0, Test 0.6\n"
     ]
    }
   ],
   "source": [
    "print(f'Train {acc_train}, Test {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisi logica di utilizzo:\n",
    "\n",
    "- il training è OK?\n",
    "- si -> il test è OK? si -> buon modello no -> overfitting\n",
    "- no -> underfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7473d73b0517ab6f64aa731f2cb80c0acf19c95420055020da77d830c23b4531"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
